{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9axA6bwRLMs"
   },
   "source": [
    "# CS5660 Final Project\n",
    "## Topic Modeling on arXiv abstract data\n",
    "Ryan Dielhenn  \n",
    "Joe Jimenez  \n",
    "Bohdan Hrotovytskyy  \n",
    "Ryan Goshorn  \n",
    "CalStateLA\n",
    "\n",
    "## Topic Modeling\n",
    "\n",
    "### Def 1.\n",
    "**Topic modeling** is an **unsupervised machine learning technique** that automatically identifies the abstract topics present within a collection of documents. It assumes that each document is a mixture of a small number of topics and that each topic is characterized by a distribution over words. The goal of topic modeling is to uncover the hidden thematic structure in large textual datasets, facilitating tasks such as organization, summarization, and discovery of patterns without prior annotation.\n",
    "\n",
    "### Def 2.\n",
    "* The problem of modeling text corpora and other collections of discrete data. The goal is to find short descriptions of the members of a collection that enable efficient processing of large collections while preserving the essential statistical relationships that are useful for basic tasks such as classification, novelty detection, summarization, and similarity and relevance judgments.\n",
    "\n",
    "\n",
    "[1] Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xR4qmlDJRLMt"
   },
   "source": [
    "### Traditional Topic Models and Their Limitation\n",
    "\n",
    "* Traditional Approaches:\n",
    "    * Latent Dirichlet Allocation (LDA)\n",
    "    * Non-Negative Matrix Factorization (NMF)\n",
    "\n",
    "* Bag-of-Words Assumption\n",
    "    * Treat documents as a collection of individual words (e.g., ignores word order).\n",
    "\n",
    "* Limitation:\n",
    "    * Ignores the meaning and relationship between words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0LORjd5RLMt"
   },
   "source": [
    "### What is a Bag-of-Words?\n",
    "* A bag-of-words is a representation of text that describes the occurrence of words within a document.\n",
    "    \n",
    "    * A vocabulary\n",
    "    * A measure of the presence of known words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4xUxfjG4GM3"
   },
   "source": [
    "# Project: Topic Modeling arXiv cs.AI with BERTopic and LLMs\n",
    "\n",
    "## Goal\n",
    "Discover the main research themes in the **cs.AI** category on arXiv by:\n",
    "- Grouping similar paper abstracts into topics\n",
    "- Automatically generating human-readable labels for each topic\n",
    "- Visualizing how topics relate to each other\n",
    "\n",
    "## Methods (High-Level)\n",
    "- **Bag-of-Words demo:** Simple example to introduce topic modeling.\n",
    "- **BERTopic:** Uses sentence embeddings + UMAP + HDBSCAN to create dense, meaningful clusters.\n",
    "- **LLM labeling (Llama3 via Ollama):** Generates concise, human-style topic names from keywords and representative documents.\n",
    "- **Visualization:**\n",
    "  - Intertopic distance maps\n",
    "  - Topic word score bar charts\n",
    "  - Document map scatter plots\n",
    "  - Final radial topic map (DataMapPlot)\n",
    "\n",
    "## Dataset\n",
    "- Source: arXiv API, category **cs.AI**\n",
    "- Data: ~1000 paper abstracts (title + abstract text)\n",
    "- Use case: Explore what kinds of AI research areas are most common in this category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15296,
     "status": "ok",
     "timestamp": 1764126182273,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "PRZGM_MJvJYV",
    "outputId": "f31f4c96-1033-473c-9ffa-26df060ce25e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of word sentence 1:\n",
      "{'likes': 2, 'movies': 2, 'john': 1, 'to': 1, 'watch': 1, 'mary': 1, 'too': 1}\n",
      "{'likes': 1, 'movies': 2, 'john': 3, 'to': 4, 'watch': 5, 'mary': 6, 'too': 7}\n",
      "We found 7 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from typing import List\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentence = [\"John likes to watch movies. Mary likes movies too.\"]\n",
    "\n",
    "\n",
    "def print_bow(sentence: List[str]) -> None:\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentence)\n",
    "    sequences = tokenizer.texts_to_sequences(sentence)\n",
    "    word_index = tokenizer.word_index\n",
    "    bow = {}\n",
    "    for key in word_index:\n",
    "        bow[key] = sequences[0].count(word_index[key])\n",
    "\n",
    "    print(f\"Bag of word sentence 1:\\n{bow}\")\n",
    "    print(f\"{word_index}\")\n",
    "    print(f\"We found {len(word_index)} unique tokens.\")\n",
    "\n",
    "\n",
    "print_bow(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1764126182273,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "f7VL5gtsvSy0",
    "outputId": "de0942e4-fc13-4a0f-e174-58a0beba8510"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John likes to watch movies. Mary likes movies too.\n"
     ]
    }
   ],
   "source": [
    "print(\"John likes to watch movies. Mary likes movies too.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wH7na1tIRLMt"
   },
   "source": [
    "### BERTopic [(ğŸ”—)](https://maartengr.github.io/BERTopic/index.html)\n",
    "BERTopic is a topic modeling technique that leverages ğŸ¤— transformers and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions.\n",
    "\n",
    "### Visual Overview\n",
    "\n",
    "BERTopic can be viewed as a sequence of steps to create its topic representations. BERTopic generates topics from text through a four-step process:\n",
    "\n",
    "1. **Embedding**: Each document is first transformed into a numerical vector using a pre-trained language model such as BERT. This step captures the semantic meaning and contextual nuances of the text.\n",
    "\n",
    "2. **Dimensionality Reduction**: Because the resulting vectors are high-dimensional, a dimensionality reduction technique (e.g., UMAP) is applied to simplify the representation while preserving important structure, making clustering more efficient and effective.\n",
    "\n",
    "3. **Clustering**: The reduced vectors are then clustered into groups, where each cluster corresponds to a potential topic.\n",
    "\n",
    "4. **Topic Representation**: For each cluster, BERTopic applies a technique called class-based TF-IDF to identify the key words that best characterize the topic.\n",
    "\n",
    "This end-to-end process enables BERTopic to generate clear, interpretable, and contextually rich topics, often outperforming traditional topic modeling methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIoe0NX6xOph"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oronUo91z0zK"
   },
   "source": [
    "## Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 302938,
     "status": "ok",
     "timestamp": 1764126485208,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "MaDCcduy8iG5",
    "outputId": "18ea82f4-62e1-467b-9f12-fec5f94336bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/153.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datamapplot\n",
      "  Downloading datamapplot-0.6.4-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Requirement already satisfied: colorcet in /usr/local/lib/python3.12/dist-packages (from datamapplot) (3.1.0)\n",
      "Collecting colorspacious>=1.1 (from datamapplot)\n",
      "  Downloading colorspacious-1.1.2-py2.py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dask<2025.0.1,>=2024.4.1 (from dask[complete]<2025.0.1,>=2024.4.1->datamapplot)\n",
      "  Downloading dask-2024.12.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting datashader>=0.16 (from datamapplot)\n",
      "  Downloading datashader-0.18.2-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from datamapplot) (6.5.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from datamapplot) (3.1.6)\n",
      "Requirement already satisfied: numba>=0.56 in /usr/local/lib/python3.12/dist-packages (from datamapplot) (0.60.0)\n",
      "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.12/dist-packages (from datamapplot) (2.0.2)\n",
      "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.12/dist-packages (from datamapplot) (2.2.2)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (from datamapplot) (18.1.0)\n",
      "Collecting pylabeladjust (from datamapplot)\n",
      "  Downloading pylabeladjust-0.1.13-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from datamapplot) (2.32.4)\n",
      "Collecting rcssmin>=1.1.2 (from datamapplot)\n",
      "  Downloading rcssmin-1.2.2-cp312-cp312-manylinux1_x86_64.whl.metadata (4.8 kB)\n",
      "Collecting rjsmin>=1.2.2 (from datamapplot)\n",
      "  Downloading rjsmin-1.2.5-cp312-cp312-manylinux1_x86_64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: scikit-image>=0.22 in /usr/local/lib/python3.12/dist-packages (from datamapplot) (0.25.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1 in /usr/local/lib/python3.12/dist-packages (from datamapplot) (1.6.1)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from datamapplot) (4.5.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from datamapplot) (4.15.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.12/dist-packages (from dask<2025.0.1,>=2024.4.1->dask[complete]<2025.0.1,>=2024.4.1->datamapplot) (8.3.1)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from dask<2025.0.1,>=2024.4.1->dask[complete]<2025.0.1,>=2024.4.1->datamapplot) (3.1.2)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.12/dist-packages (from dask<2025.0.1,>=2024.4.1->dask[complete]<2025.0.1,>=2024.4.1->datamapplot) (2025.3.0)\n",
      "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from dask<2025.0.1,>=2024.4.1->dask[complete]<2025.0.1,>=2024.4.1->datamapplot) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from dask<2025.0.1,>=2024.4.1->dask[complete]<2025.0.1,>=2024.4.1->datamapplot) (6.0.3)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from dask<2025.0.1,>=2024.4.1->dask[complete]<2025.0.1,>=2024.4.1->datamapplot) (0.12.1)\n",
      "Collecting lz4>=4.3.2 (from dask[complete]<2025.0.1,>=2024.4.1->datamapplot)\n",
      "  Downloading lz4-4.4.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.12/dist-packages (from datashader>=0.16->datamapplot) (1.0.0)\n",
      "Requirement already satisfied: param in /usr/local/lib/python3.12/dist-packages (from datashader>=0.16->datamapplot) (2.3.0)\n",
      "Collecting pyct (from datashader>=0.16->datamapplot)\n",
      "  Downloading pyct-0.6.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from datashader>=0.16->datamapplot) (1.16.3)\n",
      "Requirement already satisfied: xarray in /usr/local/lib/python3.12/dist-packages (from datashader>=0.16->datamapplot) (2025.11.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.56->datamapplot) (0.43.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0->datamapplot) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0->datamapplot) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.22->datamapplot) (3.5)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.22->datamapplot) (2.37.2)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.22->datamapplot) (2025.10.16)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.22->datamapplot) (0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1->datamapplot) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1->datamapplot) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->datamapplot) (3.0.3)\n",
      "Collecting Pyqtree<2.0.0,>=1.0.0 (from pylabeladjust->datamapplot)\n",
      "  Downloading Pyqtree-1.0.0.tar.gz (5.2 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.2 in /usr/local/lib/python3.12/dist-packages (from pylabeladjust->datamapplot) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->datamapplot) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->datamapplot) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->datamapplot) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->datamapplot) (2025.11.12)\n",
      "Requirement already satisfied: locket in /usr/local/lib/python3.12/dist-packages (from partd>=1.4.0->dask<2025.0.1,>=2024.4.1->dask[complete]<2025.0.1,>=2024.4.1->datamapplot) (1.0.0)\n",
      "Collecting dask-expr<1.2,>=1.1 (from dask<2025.0.1,>=2024.4.1->dask[complete]<2025.0.1,>=2024.4.1->datamapplot)\n",
      "  Downloading dask_expr-1.1.21-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting distributed==2024.12.1 (from dask<2025.0.1,>=2024.4.1->dask[complete]<2025.0.1,>=2024.4.1->datamapplot)\n",
      "  Downloading distributed-2024.12.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: bokeh>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from dask<2025.0.1,>=2024.4.1->dask[complete]<2025.0.1,>=2024.4.1->datamapplot) (3.7.3)\n",
      "Requirement already satisfied: msgpack>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from distributed==2024.12.1->dask<2025.0.1,>=2024.4.1->dask[complete]<2025.0.1,>=2024.4.1->datamapplot) (1.1.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from distributed==2024.12.1->dask<2025.0.1,>=2024.4.1->dask[complete]<2025.0.1,>=2024.4.1->datamapplot) (5.9.5)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.12/dist-packages (from distributed==2024.12.1->dask<2025.0.1,>=2024.4.1->dask[complete]<2025.0.1,>=2024.4.1->datamapplot) (2.4.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from distributed==2024.12.1->dask<2025.0.1,>=2024.4.1->dask[complete]<2025.0.1,>=2024.4.1->datamapplot) (3.2.2)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from distributed==2024.12.1->dask<2025.0.1,>=2024.4.1->dask[complete]<2025.0.1,>=2024.4.1->datamapplot) (6.5.1)\n",
      "Requirement already satisfied: zict>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from distributed==2024.12.1->dask<2025.0.1,>=2024.4.1->dask[complete]<2025.0.1,>=2024.4.1->datamapplot) (3.0.0)\n",
      "Requirement already satisfied: narwhals>=1.13 in /usr/local/lib/python3.12/dist-packages (from bokeh>=3.1.0->dask<2025.0.1,>=2024.4.1->dask[complete]<2025.0.1,>=2024.4.1->datamapplot) (2.12.0)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.12/dist-packages (from bokeh>=3.1.0->dask<2025.0.1,>=2024.4.1->dask[complete]<2025.0.1,>=2024.4.1->datamapplot) (2025.10.0)\n",
      "Downloading datamapplot-0.6.4-py3-none-any.whl (168 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m168.5/168.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorspacious-1.1.2-py2.py3-none-any.whl (37 kB)\n",
      "Downloading dask-2024.12.1-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datashader-0.18.2-py3-none-any.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rcssmin-1.2.2-cp312-cp312-manylinux1_x86_64.whl (49 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rjsmin-1.2.5-cp312-cp312-manylinux1_x86_64.whl (31 kB)\n",
      "Downloading pylabeladjust-0.1.13-py3-none-any.whl (19 kB)\n",
      "Downloading lz4-4.4.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distributed-2024.12.1-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyct-0.6.0-py3-none-any.whl (16 kB)\n",
      "Downloading dask_expr-1.1.21-py3-none-any.whl (244 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: Pyqtree\n",
      "  Building wheel for Pyqtree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for Pyqtree: filename=Pyqtree-1.0.0-py3-none-any.whl size=5969 sha256=13ca6060fd59de4ef527e19f12c6a06d44cc6cb0fba7063980daec378aab6cc8\n",
      "  Stored in directory: /root/.cache/pip/wheels/86/02/24/506ac193949f48c8bec599b613d722bd64a83063a190b3bff7\n",
      "Successfully built Pyqtree\n",
      "Installing collected packages: rjsmin, rcssmin, Pyqtree, pyct, lz4, colorspacious, dask, pylabeladjust, distributed, dask-expr, datashader, datamapplot\n",
      "  Attempting uninstall: dask\n",
      "    Found existing installation: dask 2025.9.1\n",
      "    Uninstalling dask-2025.9.1:\n",
      "      Successfully uninstalled dask-2025.9.1\n",
      "  Attempting uninstall: distributed\n",
      "    Found existing installation: distributed 2025.9.1\n",
      "    Uninstalling distributed-2025.9.1:\n",
      "      Successfully uninstalled distributed-2025.9.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "rapids-dask-dependency 25.10.0 requires dask==2025.9.1, but you have dask 2024.12.1 which is incompatible.\n",
      "rapids-dask-dependency 25.10.0 requires distributed==2025.9.1, but you have distributed 2024.12.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Pyqtree-1.0.0 colorspacious-1.1.2 dask-2024.12.1 dask-expr-1.1.21 datamapplot-0.6.4 datashader-0.18.2 distributed-2024.12.1 lz4-4.4.5 pyct-0.6.0 pylabeladjust-0.1.13 rcssmin-1.2.2 rjsmin-1.2.5\n",
      "Hit:1 https://cli.github.com/packages stable InRelease\n",
      "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
      "Get:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
      "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
      "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,486 kB]\n",
      "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
      "Get:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n",
      "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,539 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,596 kB]\n",
      "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,153 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,876 kB]\n",
      "Get:18 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,008 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,222 kB]\n",
      "Get:20 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,290 kB]\n",
      "Get:21 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,834 kB]\n",
      "Fetched 37.5 MB in 5s (7,606 kB/s)\n",
      "Reading package lists... Done\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  at-spi2-core cuda-drivers-580 dkms fakeroot gsettings-desktop-schemas\n",
      "  keyboard-configuration libatk-bridge2.0-0 libatk1.0-0 libatk1.0-data\n",
      "  libatspi2.0-0 libfakeroot libgail-common libgail18 libgtk-3-0 libgtk-3-bin\n",
      "  libgtk-3-common libgtk2.0-0 libgtk2.0-bin libgtk2.0-common libgudev-1.0-0\n",
      "  libjansson4 liblocale-gettext-perl libnvidia-cfg1-580 libnvidia-common-580\n",
      "  libnvidia-compute-580 libnvidia-decode-580 libnvidia-encode-580\n",
      "  libnvidia-extra-580 libnvidia-fbc1-580 libnvidia-gl-580\n",
      "  libnvidia-gpucomp-580 libpci3 librsvg2-common libudev1 libxcomposite1\n",
      "  libxcvt0 libxtst6 nvidia-dkms-580 nvidia-driver-580 nvidia-firmware-580\n",
      "  nvidia-kernel-common-580 nvidia-kernel-source-580 nvidia-modprobe\n",
      "  nvidia-persistenced nvidia-settings pci.ids python3-xkit\n",
      "  screen-resolution-extra session-migration switcheroo-control\n",
      "  systemd-hwe-hwdb udev xcvt xserver-xorg-core xserver-xorg-video-nvidia-580\n",
      "Suggested packages:\n",
      "  menu gvfs nvidia-driver-pinning-580 xfonts-100dpi | xfonts-75dpi\n",
      "  xfonts-scalable\n",
      "Recommended packages:\n",
      "  libnvidia-compute-580:i386 libnvidia-decode-580:i386\n",
      "  libnvidia-encode-580:i386 libnvidia-fbc1-580:i386 libnvidia-gl-580:i386\n",
      "The following NEW packages will be installed:\n",
      "  at-spi2-core cuda-drivers cuda-drivers-580 dkms fakeroot\n",
      "  gsettings-desktop-schemas keyboard-configuration libatk-bridge2.0-0\n",
      "  libatk1.0-0 libatk1.0-data libatspi2.0-0 libfakeroot libgail-common\n",
      "  libgail18 libgtk-3-0 libgtk-3-bin libgtk-3-common libgtk2.0-0 libgtk2.0-bin\n",
      "  libgtk2.0-common libgudev-1.0-0 libjansson4 liblocale-gettext-perl\n",
      "  libnvidia-cfg1-580 libnvidia-common-580 libnvidia-compute-580\n",
      "  libnvidia-decode-580 libnvidia-encode-580 libnvidia-extra-580\n",
      "  libnvidia-fbc1-580 libnvidia-gl-580 libnvidia-gpucomp-580 libpci3\n",
      "  librsvg2-common libxcomposite1 libxcvt0 libxtst6 nvidia-dkms-580\n",
      "  nvidia-driver-580 nvidia-firmware-580 nvidia-kernel-common-580\n",
      "  nvidia-kernel-source-580 nvidia-modprobe nvidia-persistenced nvidia-settings\n",
      "  pci.ids pciutils python3-xkit screen-resolution-extra session-migration\n",
      "  switcheroo-control systemd-hwe-hwdb udev xcvt xserver-xorg-core\n",
      "  xserver-xorg-video-nvidia-580\n",
      "The following packages will be upgraded:\n",
      "  libudev1\n",
      "1 upgraded, 56 newly installed, 0 to remove and 53 not upgraded.\n",
      "Need to get 396 MB of archives.\n",
      "After this operation, 1,199 MB of additional disk space will be used.\n",
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  dkms 1:3.2.2-1ubuntu1 [53.5 kB]\n",
      "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-decode-580 580.105.08-0ubuntu1 [2,723 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblocale-gettext-perl amd64 1.07-4build3 [17.1 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 keyboard-configuration all 1.205ubuntu3 [206 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.17 [76.7 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.17 [1,557 kB]\n",
      "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-common-580 580.105.08-0ubuntu1 [27.9 kB]\n",
      "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-cfg1-580 580.105.08-0ubuntu1 [148 kB]\n",
      "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-gpucomp-580 580.105.08-0ubuntu1 [18.5 MB]\n",
      "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-persistenced 580.105.08-0ubuntu1 [82.4 kB]\n",
      "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-compute-580 580.105.08-0ubuntu1 [54.6 MB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjansson4 amd64 2.13.1-1.1build3 [32.4 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 pci.ids all 0.0~2022.01.22-1ubuntu0.1 [251 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpci3 amd64 1:3.7.0-6 [28.9 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 pciutils amd64 1:3.7.0-6 [63.6 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatspi2.0-0 amd64 2.44.0-3 [80.9 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 session-migration amd64 0.3.6 [9,774 B]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 gsettings-desktop-schemas all 42.0-1ubuntu1 [31.1 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 at-spi2-core amd64 2.44.0-3 [54.4 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcvt0 amd64 0.1.1-3 [5,494 B]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-xorg-core amd64 2:21.1.4-2ubuntu1.7~22.04.16 [1,478 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfakeroot amd64 1.28-1ubuntu1 [31.5 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 fakeroot amd64 1.28-1ubuntu1 [60.4 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-data all 2.36.0-3build1 [2,824 B]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-0 amd64 2.36.0-3build1 [51.9 kB]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-bridge2.0-0 amd64 2.38.0-3 [66.6 kB]\n",
      "Get:28 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-common all 2.24.33-2ubuntu2.1 [125 kB]\n",
      "Get:29 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcomposite1 amd64 1:0.4.5-1build2 [7,192 B]\n",
      "Get:30 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-0 amd64 2.24.33-2ubuntu2.1 [2,038 kB]\n",
      "Get:31 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail18 amd64 2.24.33-2ubuntu2.1 [15.9 kB]\n",
      "Get:32 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-gl-580 580.105.08-0ubuntu1 [148 MB]\n",
      "Get:33 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail-common amd64 2.24.33-2ubuntu2.1 [132 kB]\n",
      "Get:34 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk-3-common all 3.24.33-1ubuntu2.2 [239 kB]\n",
      "Get:35 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk-3-0 amd64 3.24.33-1ubuntu2.2 [3,053 kB]\n",
      "Get:36 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk-3-bin amd64 3.24.33-1ubuntu2.2 [69.6 kB]\n",
      "Get:37 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-bin amd64 2.24.33-2ubuntu2.1 [7,936 B]\n",
      "Get:38 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgudev-1.0-0 amd64 1:237-2build1 [16.3 kB]\n",
      "Get:39 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 librsvg2-common amd64 2.52.5+dfsg-3ubuntu0.2 [17.7 kB]\n",
      "Get:40 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-xkit all 0.5.0ubuntu5 [18.5 kB]\n",
      "Get:41 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 screen-resolution-extra all 0.18.2ubuntu0.1 [4,306 B]\n",
      "Get:42 http://archive.ubuntu.com/ubuntu jammy/main amd64 switcheroo-control amd64 2.4-3build2 [16.5 kB]\n",
      "Get:43 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.6 [3,668 B]\n",
      "Get:44 http://archive.ubuntu.com/ubuntu jammy/main amd64 xcvt amd64 0.1.1-3 [7,140 B]\n",
      "Get:45 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-kernel-source-580 580.105.08-0ubuntu1 [83.1 MB]\n",
      "Get:46 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-firmware-580 580.105.08-0ubuntu1 [74.6 MB]\n",
      "Get:47 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-modprobe 580.105.08-0ubuntu1 [14.9 kB]\n",
      "Get:48 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-kernel-common-580 580.105.08-0ubuntu1 [701 kB]\n",
      "Get:49 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-dkms-580 580.105.08-0ubuntu1 [14.7 kB]\n",
      "Get:50 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-extra-580 580.105.08-0ubuntu1 [73.1 kB]\n",
      "Get:51 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-encode-580 580.105.08-0ubuntu1 [107 kB]\n",
      "Get:52 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  xserver-xorg-video-nvidia-580 580.105.08-0ubuntu1 [1,695 kB]\n",
      "Get:53 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-fbc1-580 580.105.08-0ubuntu1 [86.9 kB]\n",
      "Get:54 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-driver-580 580.105.08-0ubuntu1 [499 kB]\n",
      "Get:55 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-drivers-580 580.105.08-0ubuntu1 [2,562 B]\n",
      "Get:56 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-drivers 580.105.08-0ubuntu1 [2,514 B]\n",
      "Get:57 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-settings 580.105.08-0ubuntu1 [924 kB]\n",
      "Fetched 396 MB in 10s (41.7 MB/s)\n",
      "Extracting templates from packages: 100%\n",
      "Preconfiguring packages ...\n",
      "Selecting previously unselected package liblocale-gettext-perl.\n",
      "(Reading database ... 121713 files and directories currently installed.)\n",
      "Preparing to unpack .../liblocale-gettext-perl_1.07-4build3_amd64.deb ...\n",
      "Unpacking liblocale-gettext-perl (1.07-4build3) ...\n",
      "Selecting previously unselected package keyboard-configuration.\n",
      "Preparing to unpack .../keyboard-configuration_1.205ubuntu3_all.deb ...\n",
      "Unpacking keyboard-configuration (1.205ubuntu3) ...\n",
      "Selecting previously unselected package dkms.\n",
      "Preparing to unpack .../dkms_1%3a3.2.2-1ubuntu1_all.deb ...\n",
      "Unpacking dkms (1:3.2.2-1ubuntu1) ...\n",
      "Preparing to unpack .../libudev1_249.11-0ubuntu3.17_amd64.deb ...\n",
      "Unpacking libudev1:amd64 (249.11-0ubuntu3.17) over (249.11-0ubuntu3.12) ...\n",
      "Setting up libudev1:amd64 (249.11-0ubuntu3.17) ...\n",
      "Selecting previously unselected package udev.\n",
      "(Reading database ... 121774 files and directories currently installed.)\n",
      "Preparing to unpack .../00-udev_249.11-0ubuntu3.17_amd64.deb ...\n",
      "Unpacking udev (249.11-0ubuntu3.17) ...\n",
      "Selecting previously unselected package libjansson4:amd64.\n",
      "Preparing to unpack .../01-libjansson4_2.13.1-1.1build3_amd64.deb ...\n",
      "Unpacking libjansson4:amd64 (2.13.1-1.1build3) ...\n",
      "Selecting previously unselected package pci.ids.\n",
      "Preparing to unpack .../02-pci.ids_0.0~2022.01.22-1ubuntu0.1_all.deb ...\n",
      "Unpacking pci.ids (0.0~2022.01.22-1ubuntu0.1) ...\n",
      "Selecting previously unselected package libpci3:amd64.\n",
      "Preparing to unpack .../03-libpci3_1%3a3.7.0-6_amd64.deb ...\n",
      "Unpacking libpci3:amd64 (1:3.7.0-6) ...\n",
      "Selecting previously unselected package pciutils.\n",
      "Preparing to unpack .../04-pciutils_1%3a3.7.0-6_amd64.deb ...\n",
      "Unpacking pciutils (1:3.7.0-6) ...\n",
      "Selecting previously unselected package libatspi2.0-0:amd64.\n",
      "Preparing to unpack .../05-libatspi2.0-0_2.44.0-3_amd64.deb ...\n",
      "Unpacking libatspi2.0-0:amd64 (2.44.0-3) ...\n",
      "Selecting previously unselected package libxtst6:amd64.\n",
      "Preparing to unpack .../06-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
      "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
      "Selecting previously unselected package session-migration.\n",
      "Preparing to unpack .../07-session-migration_0.3.6_amd64.deb ...\n",
      "Unpacking session-migration (0.3.6) ...\n",
      "Selecting previously unselected package gsettings-desktop-schemas.\n",
      "Preparing to unpack .../08-gsettings-desktop-schemas_42.0-1ubuntu1_all.deb ...\n",
      "Unpacking gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
      "Selecting previously unselected package at-spi2-core.\n",
      "Preparing to unpack .../09-at-spi2-core_2.44.0-3_amd64.deb ...\n",
      "Unpacking at-spi2-core (2.44.0-3) ...\n",
      "Selecting previously unselected package libnvidia-decode-580:amd64.\n",
      "Preparing to unpack .../10-libnvidia-decode-580_580.105.08-0ubuntu1_amd64.deb ...\n",
      "Unpacking libnvidia-decode-580:amd64 (580.105.08-0ubuntu1) ...\n",
      "Selecting previously unselected package libnvidia-common-580.\n",
      "Preparing to unpack .../11-libnvidia-common-580_580.105.08-0ubuntu1_all.deb ...\n",
      "Unpacking libnvidia-common-580 (580.105.08-0ubuntu1) ...\n",
      "Selecting previously unselected package libnvidia-cfg1-580:amd64.\n",
      "Preparing to unpack .../12-libnvidia-cfg1-580_580.105.08-0ubuntu1_amd64.deb ...\n",
      "Unpacking libnvidia-cfg1-580:amd64 (580.105.08-0ubuntu1) ...\n",
      "Selecting previously unselected package libnvidia-gpucomp-580:amd64.\n",
      "Preparing to unpack .../13-libnvidia-gpucomp-580_580.105.08-0ubuntu1_amd64.deb ...\n",
      "Unpacking libnvidia-gpucomp-580:amd64 (580.105.08-0ubuntu1) ...\n",
      "Selecting previously unselected package nvidia-persistenced.\n",
      "Preparing to unpack .../14-nvidia-persistenced_580.105.08-0ubuntu1_amd64.deb ...\n",
      "Unpacking nvidia-persistenced (580.105.08-0ubuntu1) ...\n",
      "Selecting previously unselected package libnvidia-compute-580:amd64.\n",
      "Preparing to unpack .../15-libnvidia-compute-580_580.105.08-0ubuntu1_amd64.deb ...\n",
      "Unpacking libnvidia-compute-580:amd64 (580.105.08-0ubuntu1) ...\n",
      "Selecting previously unselected package libnvidia-gl-580:amd64.\n",
      "Preparing to unpack .../16-libnvidia-gl-580_580.105.08-0ubuntu1_amd64.deb ...\n",
      "dpkg-query: no packages found matching libnvidia-gl-535\n",
      "Unpacking libnvidia-gl-580:amd64 (580.105.08-0ubuntu1) ...\n",
      "Selecting previously unselected package nvidia-kernel-source-580.\n",
      "Preparing to unpack .../17-nvidia-kernel-source-580_580.105.08-0ubuntu1_amd64.deb ...\n",
      "Unpacking nvidia-kernel-source-580 (580.105.08-0ubuntu1) ...\n",
      "Selecting previously unselected package nvidia-firmware-580.\n",
      "Preparing to unpack .../18-nvidia-firmware-580_580.105.08-0ubuntu1_amd64.deb ...\n",
      "Unpacking nvidia-firmware-580 (580.105.08-0ubuntu1) ...\n",
      "Selecting previously unselected package nvidia-modprobe.\n",
      "Preparing to unpack .../19-nvidia-modprobe_580.105.08-0ubuntu1_amd64.deb ...\n",
      "Unpacking nvidia-modprobe (580.105.08-0ubuntu1) ...\n",
      "Selecting previously unselected package nvidia-kernel-common-580.\n",
      "Preparing to unpack .../20-nvidia-kernel-common-580_580.105.08-0ubuntu1_amd64.deb ...\n",
      "Unpacking nvidia-kernel-common-580 (580.105.08-0ubuntu1) ...\n",
      "Selecting previously unselected package nvidia-dkms-580.\n",
      "Preparing to unpack .../21-nvidia-dkms-580_580.105.08-0ubuntu1_amd64.deb ...\n",
      "Unpacking nvidia-dkms-580 (580.105.08-0ubuntu1) ...\n",
      "Selecting previously unselected package libnvidia-extra-580:amd64.\n",
      "Preparing to unpack .../22-libnvidia-extra-580_580.105.08-0ubuntu1_amd64.deb ...\n",
      "Unpacking libnvidia-extra-580:amd64 (580.105.08-0ubuntu1) ...\n",
      "Selecting previously unselected package libnvidia-encode-580:amd64.\n",
      "Preparing to unpack .../23-libnvidia-encode-580_580.105.08-0ubuntu1_amd64.deb ...\n",
      "Unpacking libnvidia-encode-580:amd64 (580.105.08-0ubuntu1) ...\n",
      "Selecting previously unselected package libxcvt0:amd64.\n",
      "Preparing to unpack .../24-libxcvt0_0.1.1-3_amd64.deb ...\n",
      "Unpacking libxcvt0:amd64 (0.1.1-3) ...\n",
      "Selecting previously unselected package xserver-xorg-core.\n",
      "Preparing to unpack .../25-xserver-xorg-core_2%3a21.1.4-2ubuntu1.7~22.04.16_amd64.deb ...\n",
      "Unpacking xserver-xorg-core (2:21.1.4-2ubuntu1.7~22.04.16) ...\n",
      "Selecting previously unselected package xserver-xorg-video-nvidia-580.\n",
      "Preparing to unpack .../26-xserver-xorg-video-nvidia-580_580.105.08-0ubuntu1_amd64.deb ...\n",
      "Unpacking xserver-xorg-video-nvidia-580 (580.105.08-0ubuntu1) ...\n",
      "Selecting previously unselected package libnvidia-fbc1-580:amd64.\n",
      "Preparing to unpack .../27-libnvidia-fbc1-580_580.105.08-0ubuntu1_amd64.deb ...\n",
      "Unpacking libnvidia-fbc1-580:amd64 (580.105.08-0ubuntu1) ...\n",
      "Selecting previously unselected package nvidia-driver-580.\n",
      "Preparing to unpack .../28-nvidia-driver-580_580.105.08-0ubuntu1_amd64.deb ...\n",
      "Unpacking nvidia-driver-580 (580.105.08-0ubuntu1) ...\n",
      "Selecting previously unselected package cuda-drivers-580.\n",
      "Preparing to unpack .../29-cuda-drivers-580_580.105.08-0ubuntu1_amd64.deb ...\n",
      "Unpacking cuda-drivers-580 (580.105.08-0ubuntu1) ...\n",
      "Selecting previously unselected package cuda-drivers.\n",
      "Preparing to unpack .../30-cuda-drivers_580.105.08-0ubuntu1_amd64.deb ...\n",
      "Unpacking cuda-drivers (580.105.08-0ubuntu1) ...\n",
      "Selecting previously unselected package libfakeroot:amd64.\n",
      "Preparing to unpack .../31-libfakeroot_1.28-1ubuntu1_amd64.deb ...\n",
      "Unpacking libfakeroot:amd64 (1.28-1ubuntu1) ...\n",
      "Selecting previously unselected package fakeroot.\n",
      "Preparing to unpack .../32-fakeroot_1.28-1ubuntu1_amd64.deb ...\n",
      "Unpacking fakeroot (1.28-1ubuntu1) ...\n",
      "Selecting previously unselected package libatk1.0-data.\n",
      "Preparing to unpack .../33-libatk1.0-data_2.36.0-3build1_all.deb ...\n",
      "Unpacking libatk1.0-data (2.36.0-3build1) ...\n",
      "Selecting previously unselected package libatk1.0-0:amd64.\n",
      "Preparing to unpack .../34-libatk1.0-0_2.36.0-3build1_amd64.deb ...\n",
      "Unpacking libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
      "Selecting previously unselected package libatk-bridge2.0-0:amd64.\n",
      "Preparing to unpack .../35-libatk-bridge2.0-0_2.38.0-3_amd64.deb ...\n",
      "Unpacking libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
      "Selecting previously unselected package libgtk2.0-common.\n",
      "Preparing to unpack .../36-libgtk2.0-common_2.24.33-2ubuntu2.1_all.deb ...\n",
      "Unpacking libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
      "Selecting previously unselected package libxcomposite1:amd64.\n",
      "Preparing to unpack .../37-libxcomposite1_1%3a0.4.5-1build2_amd64.deb ...\n",
      "Unpacking libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
      "Selecting previously unselected package libgtk2.0-0:amd64.\n",
      "Preparing to unpack .../38-libgtk2.0-0_2.24.33-2ubuntu2.1_amd64.deb ...\n",
      "Unpacking libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
      "Selecting previously unselected package libgail18:amd64.\n",
      "Preparing to unpack .../39-libgail18_2.24.33-2ubuntu2.1_amd64.deb ...\n",
      "Unpacking libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
      "Selecting previously unselected package libgail-common:amd64.\n",
      "Preparing to unpack .../40-libgail-common_2.24.33-2ubuntu2.1_amd64.deb ...\n",
      "Unpacking libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
      "Selecting previously unselected package libgtk-3-common.\n",
      "Preparing to unpack .../41-libgtk-3-common_3.24.33-1ubuntu2.2_all.deb ...\n",
      "Unpacking libgtk-3-common (3.24.33-1ubuntu2.2) ...\n",
      "Selecting previously unselected package libgtk-3-0:amd64.\n",
      "Preparing to unpack .../42-libgtk-3-0_3.24.33-1ubuntu2.2_amd64.deb ...\n",
      "Unpacking libgtk-3-0:amd64 (3.24.33-1ubuntu2.2) ...\n",
      "Selecting previously unselected package libgtk-3-bin.\n",
      "Preparing to unpack .../43-libgtk-3-bin_3.24.33-1ubuntu2.2_amd64.deb ...\n",
      "Unpacking libgtk-3-bin (3.24.33-1ubuntu2.2) ...\n",
      "Selecting previously unselected package libgtk2.0-bin.\n",
      "Preparing to unpack .../44-libgtk2.0-bin_2.24.33-2ubuntu2.1_amd64.deb ...\n",
      "Unpacking libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
      "Selecting previously unselected package libgudev-1.0-0:amd64.\n",
      "Preparing to unpack .../45-libgudev-1.0-0_1%3a237-2build1_amd64.deb ...\n",
      "Unpacking libgudev-1.0-0:amd64 (1:237-2build1) ...\n",
      "Selecting previously unselected package librsvg2-common:amd64.\n",
      "Preparing to unpack .../46-librsvg2-common_2.52.5+dfsg-3ubuntu0.2_amd64.deb ...\n",
      "Unpacking librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
      "Selecting previously unselected package python3-xkit.\n",
      "Preparing to unpack .../47-python3-xkit_0.5.0ubuntu5_all.deb ...\n",
      "Unpacking python3-xkit (0.5.0ubuntu5) ...\n",
      "Selecting previously unselected package screen-resolution-extra.\n",
      "Preparing to unpack .../48-screen-resolution-extra_0.18.2ubuntu0.1_all.deb ...\n",
      "Unpacking screen-resolution-extra (0.18.2ubuntu0.1) ...\n",
      "Selecting previously unselected package nvidia-settings.\n",
      "Preparing to unpack .../49-nvidia-settings_580.105.08-0ubuntu1_amd64.deb ...\n",
      "Unpacking nvidia-settings (580.105.08-0ubuntu1) ...\n",
      "Selecting previously unselected package switcheroo-control.\n",
      "Preparing to unpack .../50-switcheroo-control_2.4-3build2_amd64.deb ...\n",
      "Unpacking switcheroo-control (2.4-3build2) ...\n",
      "Selecting previously unselected package systemd-hwe-hwdb.\n",
      "Preparing to unpack .../51-systemd-hwe-hwdb_249.11.6_all.deb ...\n",
      "Unpacking systemd-hwe-hwdb (249.11.6) ...\n",
      "Selecting previously unselected package xcvt.\n",
      "Preparing to unpack .../52-xcvt_0.1.1-3_amd64.deb ...\n",
      "Unpacking xcvt (0.1.1-3) ...\n",
      "Setting up session-migration (0.3.6) ...\n",
      "Created symlink /etc/systemd/user/graphical-session-pre.target.wants/session-migration.service â†’ /usr/lib/systemd/user/session-migration.service.\n",
      "Setting up pci.ids (0.0~2022.01.22-1ubuntu0.1) ...\n",
      "Setting up libnvidia-gpucomp-580:amd64 (580.105.08-0ubuntu1) ...\n",
      "Setting up libnvidia-fbc1-580:amd64 (580.105.08-0ubuntu1) ...\n",
      "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
      "Setting up libatspi2.0-0:amd64 (2.44.0-3) ...\n",
      "Setting up nvidia-kernel-source-580 (580.105.08-0ubuntu1) ...\n",
      "Setting up libfakeroot:amd64 (1.28-1ubuntu1) ...\n",
      "Setting up libjansson4:amd64 (2.13.1-1.1build3) ...\n",
      "Setting up dkms (1:3.2.2-1ubuntu1) ...\n",
      "Setting up nvidia-modprobe (580.105.08-0ubuntu1) ...\n",
      "Setting up fakeroot (1.28-1ubuntu1) ...\n",
      "update-alternatives: using /usr/bin/fakeroot-sysv to provide /usr/bin/fakeroot (fakeroot) in auto mode\n",
      "Setting up libnvidia-extra-580:amd64 (580.105.08-0ubuntu1) ...\n",
      "Setting up libnvidia-common-580 (580.105.08-0ubuntu1) ...\n",
      "Setting up nvidia-firmware-580 (580.105.08-0ubuntu1) ...\n",
      "Setting up udev (249.11-0ubuntu3.17) ...\n",
      "invoke-rc.d: could not determine current runlevel\n",
      "invoke-rc.d: policy-rc.d denied execution of start.\n",
      "Setting up librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
      "Setting up systemd-hwe-hwdb (249.11.6) ...\n",
      "Setting up libatk1.0-data (2.36.0-3build1) ...\n",
      "Setting up libpci3:amd64 (1:3.7.0-6) ...\n",
      "Setting up libnvidia-cfg1-580:amd64 (580.105.08-0ubuntu1) ...\n",
      "Setting up libxcvt0:amd64 (0.1.1-3) ...\n",
      "Setting up libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
      "Setting up libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
      "Setting up python3-xkit (0.5.0ubuntu5) ...\n",
      "Setting up libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
      "Setting up liblocale-gettext-perl (1.07-4build3) ...\n",
      "Setting up libgudev-1.0-0:amd64 (1:237-2build1) ...\n",
      "Setting up libgtk-3-common (3.24.33-1ubuntu2.2) ...\n",
      "Setting up gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
      "Setting up nvidia-persistenced (580.105.08-0ubuntu1) ...\n",
      "Warning: The home dir /var/run/nvidia-persistenced/ you specified can't be accessed: No such file or directory\n",
      "Adding system user `nvidia-persistenced' (UID 104) ...\n",
      "Adding new group `nvidia-persistenced' (GID 111) ...\n",
      "Adding new user `nvidia-persistenced' (UID 104) with group `nvidia-persistenced' ...\n",
      "Not creating home directory `/var/run/nvidia-persistenced/'.\n",
      "Created symlink /etc/systemd/system/multi-user.target.wants/nvidia-persistenced.service â†’ /lib/systemd/system/nvidia-persistenced.service.\n",
      "Setting up libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
      "Setting up xcvt (0.1.1-3) ...\n",
      "Setting up libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
      "Setting up nvidia-kernel-common-580 (580.105.08-0ubuntu1) ...\n",
      "Setting up nvidia-dkms-580 (580.105.08-0ubuntu1) ...\n",
      "Loading new nvidia/580.105.08 DKMS files...\n",
      "Building for 5.15.0-161-generic\n",
      "\n",
      "Building initial module nvidia/580.105.08 for 5.15.0-161-generic\n",
      "Sign command: /usr/src/linux-headers-5.15.0-161-generic/scripts/sign-file\n",
      "Binary update-secureboot-policy not found, modules won't be signed\n",
      "\n",
      "Building module(s)....................................................................... done.\n",
      "Installing /lib/modules/5.15.0-161-generic/updates/dkms/nvidia.ko\n",
      "Installing /lib/modules/5.15.0-161-generic/updates/dkms/nvidia-modeset.ko\n",
      "Installing /lib/modules/5.15.0-161-generic/updates/dkms/nvidia-drm.ko\n",
      "Installing /lib/modules/5.15.0-161-generic/updates/dkms/nvidia-uvm.ko\n",
      "Installing /lib/modules/5.15.0-161-generic/updates/dkms/nvidia-peermem.ko\n",
      "Running depmod... done.\n",
      "Setting up pciutils (1:3.7.0-6) ...\n",
      "Setting up screen-resolution-extra (0.18.2ubuntu0.1) ...\n",
      "Setting up libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
      "Setting up libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
      "Setting up switcheroo-control (2.4-3build2) ...\n",
      "Created symlink /etc/systemd/system/graphical.target.wants/switcheroo-control.service â†’ /lib/systemd/system/switcheroo-control.service.\n",
      "Setting up keyboard-configuration (1.205ubuntu3) ...\n",
      "Your console font configuration will be updated the next time your system\n",
      "boots. If you want to update it now, run 'setupcon' from a virtual console.\n",
      "Setting up libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
      "Setting up xserver-xorg-core (2:21.1.4-2ubuntu1.7~22.04.16) ...\n",
      "Setting up xserver-xorg-video-nvidia-580 (580.105.08-0ubuntu1) ...\n",
      "Setting up libnvidia-decode-580:amd64 (580.105.08-0ubuntu1) ...\n",
      "Setting up libnvidia-compute-580:amd64 (580.105.08-0ubuntu1) ...\n",
      "Setting up libnvidia-encode-580:amd64 (580.105.08-0ubuntu1) ...\n",
      "Setting up libnvidia-gl-580:amd64 (580.105.08-0ubuntu1) ...\n",
      "Setting up nvidia-driver-580 (580.105.08-0ubuntu1) ...\n",
      "Setting up cuda-drivers-580 (580.105.08-0ubuntu1) ...\n",
      "Setting up cuda-drivers (580.105.08-0ubuntu1) ...\n",
      "Processing triggers for libgdk-pixbuf-2.0-0:amd64 (2.42.8+dfsg-1ubuntu0.4) ...\n",
      "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
      "Processing triggers for libglib2.0-0:amd64 (2.72.4-0ubuntu2.6) ...\n",
      "Setting up libgtk-3-0:amd64 (3.24.33-1ubuntu2.2) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
      "\n",
      "Setting up libgtk-3-bin (3.24.33-1ubuntu2.2) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
      "Setting up at-spi2-core (2.44.0-3) ...\n",
      "Setting up nvidia-settings (580.105.08-0ubuntu1) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BERTopic library\n",
    "!pip install -q BERTopic\n",
    "\n",
    "# Visualization Libraries\n",
    "!pip install datamapplot matplotlib\n",
    "\n",
    "# Tokenization and ollama for running llm locally\n",
    "!pip install -q openai tiktoken ollama\n",
    "\n",
    "# Cuda Drivers for running LLM on colab\n",
    "!apt-get update && apt-get install -y pciutils cuda-drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JetXAq7z6Z_"
   },
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 144660,
     "status": "error",
     "timestamp": 1764126629878,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "rNaWS2IizN44",
    "outputId": "e612fb26-c075-4076-dc36-70b9df5e4109"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/hdbscan/robust_single_linkage_.py:175: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  $max \\{ core_k(a), core_k(b), 1/\\alpha d(a,b) \\}$.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "errorDetails": {
      "actions": [
       {
        "action": "open_url",
        "actionText": "Open Examples",
        "url": "/notebooks/snippets/importing_libraries.ipynb"
       }
      ]
     },
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-308280725.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Utilities for fetching abstracts from the arXiv api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/CSULA/CS5660/arXiv_topic_modeling\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_arxiv_abstracts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"âœ… Imports successful!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Core\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import subprocess\n",
    "import ast\n",
    "\n",
    "# Data\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Topic modeling\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import (\n",
    "    KeyBERTInspired,\n",
    "    MaximalMarginalRelevance,\n",
    "    TextGeneration,\n",
    "    OpenAI as RepresentationOpenAI\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import matplotlib.pyplot as plt\n",
    "import datamapplot\n",
    "\n",
    "# External services\n",
    "from google.colab import drive\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "\n",
    "# Mount Google Drive to access files\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Utilities for fetching abstracts from the arXiv api\n",
    "sys.path.insert(0, \"/content/drive/MyDrive/CSULA/CS5660/arXiv_topic_modeling\")\n",
    "from utils import fetch_arxiv_abstracts\n",
    "print(\"âœ… Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN9WpI8KRLMu"
   },
   "source": [
    "## BERTopic Quick Start\n",
    "\n",
    "### ğŸ“„ Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 463098,
     "status": "aborted",
     "timestamp": 1764126629897,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "7BlZOPOEEwsR"
   },
   "outputs": [],
   "source": [
    "docs, titles = fetch_arxiv_abstracts(category='cs.AI', max_results=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "aborted",
     "timestamp": 1764126629899,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "__caSuMD8wYB"
   },
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQuAsXuSRLMu"
   },
   "source": [
    "`fetch_arXiv_abstracts` is a function from `utils/` that will return a certain number of abstracts from a category eg `cs.AI`. The arXiv api seems to have rate limiting so we may need a delay before fetching more data in the future.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwOxwGLZRLMu"
   },
   "source": [
    "### Building and Training the BERTopic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 0,
     "status": "aborted",
     "timestamp": 1764126629900,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "GIqL7aMGRLMu"
   },
   "outputs": [],
   "source": [
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSgmkjv6RLMu"
   },
   "source": [
    "After generating topics and their probabilities, we can access the frequent topics that were generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 463096,
     "status": "aborted",
     "timestamp": 1764126629901,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "tBoqMES8cg8D"
   },
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjaGPCVich-d"
   },
   "source": [
    "* -1 refers to all outliers and should typically be ignored.\n",
    "* Next, let's take a look at the most frequent topic that was generated, topic 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 0,
     "status": "aborted",
     "timestamp": 1764126629902,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "zYrvuDrzcp2I"
   },
   "outputs": [],
   "source": [
    "topic_model.get_topic(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVnVbAn_eN0B"
   },
   "source": [
    "Using `.get_document_info`, we can also extract information on a document level, such as their corresponding topics, probabilities, whether they are representative documents for a topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 463094,
     "status": "aborted",
     "timestamp": 1764126629903,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "OlAsssGfeOVe"
   },
   "outputs": [],
   "source": [
    "topic_model.get_document_info(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzMRIU4cRLMv"
   },
   "source": [
    " ### Representation Models: Fine-tune Topic Representation\n",
    "\n",
    "BERTopic uses a Bag-of-Words approach with class-based TF-IDF (c-TF-IDF) to quickly generate topic keywords without needing to re-train the model after clustering.\n",
    "While this provides good initial topic representations, BERTopic also offers optional representation models for further fine-tuning.\n",
    "These models can range from powerful GPT-like models to faster keyword extraction methods like KeyBERT, giving users flexibility to enhance topic quality as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5MaTsy3RLMv"
   },
   "source": [
    "### LLM & Generative AI\n",
    "\n",
    "Using LLMs such as GPT-4, and open source soultion, we can fine-tune topics to generate labels, summaries of the topics.\n",
    "\n",
    "- Generate a set of keywords and documetns that describe a topic best using BERTopic's c-TF-IDF .\n",
    "- Candidate keywords and documents are passed to the text generation model and asked to generate output that fits the topic best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OatGS1vuRLMv"
   },
   "source": [
    "#### Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 463093,
     "status": "aborted",
     "timestamp": 1764126629904,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "qzRLf3TGRLMv"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "I have topic that contains the following documents: \\n[DOCUMENTS]\n",
    "The topic is described by the following keywords: [KEYWORDS]\n",
    "\n",
    "Based on the above information, can you give a short label of the topic?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zg6s-1ULRLMv"
   },
   "source": [
    "### Selecting Documents\n",
    "\n",
    "Four of the most representative documents will be passed to `[Documents]`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5J5MztY-RLMv"
   },
   "source": [
    "BERTopic works rather straightforward. It consists of 5 sequential steps: embedding documents, reducing embeddings in dimensionality, cluster embeddings, tokenize documents per cluster, and finally extract the best representing words per topic.\n",
    "<br>\n",
    "<div>\n",
    "<img src=\"https://github.com/MaartenGr/BERTopic/assets/25746895/e9b0d8cf-2e19-4bf1-beb4-4ff2d9fa5e2d\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 463091,
     "status": "aborted",
     "timestamp": 1764126629904,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "xMX28Q9Tj7TO"
   },
   "outputs": [],
   "source": [
    "!curl -fsSL https://ollama.ai/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 463091,
     "status": "aborted",
     "timestamp": 1764126629905,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "jEFu_kj5kznq"
   },
   "outputs": [],
   "source": [
    "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
    "\n",
    "def run_ollama_serve():\n",
    "    subprocess.Popen([\"nohup\", \"ollama\", \"serve\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "run_ollama_serve()\n",
    "time.sleep(5)\n",
    "print(\"Ollama server started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 463091,
     "status": "aborted",
     "timestamp": 1764126629906,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "iFsrgwUuk1Tq"
   },
   "outputs": [],
   "source": [
    "!ollama pull llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 463089,
     "status": "aborted",
     "timestamp": 1764126629906,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "R3HqnuLJmDPz",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Configure the client to use the local Ollama server\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1',\n",
    "    api_key='ollama', # dummy API key required by the client library\n",
    ")\n",
    "\n",
    "# Use the model you pulled (e.g., \"llama3\")\n",
    "model_name = \"llama3\"\n",
    "\n",
    "print(f\"Sending request to {model_name}...\")\n",
    "\n",
    "# Example using the standard OpenAI client chat completion\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Explain how to run an LLM locally in one sentence.\"}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    print(\"\\n--- Model Response ---\")\n",
    "    print(response.choices[0].message.content)\n",
    "    print(\"----------------------\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")\n",
    "    print(\"Make sure the 'ollama serve' process is running in the background.\")\n",
    "\n",
    "# You can run !ollama ps again after this code executes to see the model usage\n",
    "time.sleep(2)\n",
    "!ollama ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 463089,
     "status": "aborted",
     "timestamp": 1764126629907,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "o3tNrNtORLMv"
   },
   "outputs": [],
   "source": [
    "!curl http://localhost:11434/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 463089,
     "status": "aborted",
     "timestamp": 1764126629908,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "qckIktwOtTjS"
   },
   "outputs": [],
   "source": [
    "# Assuming bertopic and its dependencies are installed\n",
    "# If not, run this line first: !pip install bertopic sentence-transformers\n",
    "\n",
    "prompt = \"\"\"\n",
    "I have a topic that contains the following documents:\n",
    "[DOCUMENTS]\n",
    "\n",
    "The topic is described by the following keywords: [KEYWORDS]\n",
    "\n",
    "Generate a concise topic label (3-7 words) that captures the main theme.\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "- Output ONLY the topic label itself\n",
    "- Do NOT include phrases like \"Here is\", \"The topic is\", \"Topic:\", or any preamble\n",
    "- Do NOT add explanations or formatting\n",
    "- Just output the label directly as plain text\n",
    "\n",
    "Example output: \"Neural Networks for Computer Vision\"\n",
    "\n",
    "Your label:\"\"\"\n",
    "\n",
    "# Configure the client to use the local Ollama server\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama_key_placeholder\",\n",
    ")\n",
    "\n",
    "# Use the model you pulled (e.g., \"llama3\")\n",
    "OLLAMA_MODEL_NAME = \"llama3\"\n",
    "ollama_representation_model = RepresentationOpenAI(client, prompt=prompt, model=OLLAMA_MODEL_NAME, delay_in_seconds=10)\n",
    "\n",
    "print(f\"Representation model configured using local Ollama model: {OLLAMA_MODEL_NAME}\")\n",
    "\n",
    "# You can now proceed with your BERTopic workflow:\n",
    "# topic_model = BERTopic(representation_model=representation_model)\n",
    "# documents = [...] # Your actual list of documents\n",
    "# topics, probabilites = topic_model.fit_transform(documents)\n",
    "\n",
    "# Verification using a simple prompt\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=OLLAMA_MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Confirm that you are running locally via Ollama.\"}\n",
    "        ],\n",
    "    )\n",
    "    print(\"\\n--- Verification Response ---\")\n",
    "    print(response.choices[0].message.content)\n",
    "    print(\"-----------------------------\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during verification: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KMup1RxRLMv"
   },
   "source": [
    "## **Preparing Embeddings**\n",
    "\n",
    "By pre-calculating the embeddings for each document, we can speed-up additional exploration steps and use the embeddings to quickly iterate over BERTopic's hyperparameters if needed.\n",
    "\n",
    "ğŸ”¥ **TIP**: You can find a great overview of good embeddings for clustering on the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 463088,
     "status": "aborted",
     "timestamp": 1764126629909,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "ehxoHWOsRLMv"
   },
   "outputs": [],
   "source": [
    "# Pre-calculate embeddings\n",
    "embedding_model = SentenceTransformer(\"BAAI/bge-small-en\")\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 463087,
     "status": "aborted",
     "timestamp": 1764126629909,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "nA_gAwY4RLMv"
   },
   "outputs": [],
   "source": [
    "#Pre-reduce embeddings for visualization purposes\n",
    "reduced_embeddings = UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine', random_state=42).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 463104,
     "status": "aborted",
     "timestamp": 1764126629928,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "lgD6_z03RLMv"
   },
   "outputs": [],
   "source": [
    "df_plot = pd.DataFrame({\n",
    "    \"x1\": [point[0] for point in reduced_embeddings],\n",
    "    \"x2\": [point[1] for point in reduced_embeddings],\n",
    "    \"docs\": docs,\n",
    "})\n",
    "\n",
    "df_plot[\"docs_short\"] = df_plot[\"docs\"].str[:100] + \"...\"\n",
    "df_plot.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 463104,
     "status": "aborted",
     "timestamp": 1764126629930,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "XaVgK4awRLMv"
   },
   "outputs": [],
   "source": [
    "pio.renderers.default = \"colab\"\n",
    "\n",
    "total_docs = len(df_plot)\n",
    "fig = px.scatter(df_plot, x=\"x1\", y=\"x2\",  hover_data=[\"docs_short\"])\n",
    "fig.update_traces(marker=dict(line=dict(width=0.5, color='white')))\n",
    "fig.update_layout(\n",
    "    title=f\"arXiv abstracts from cs.AI - Document Map ({total_docs} documents)\",\n",
    "    title_font_size=20\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5Bw6Xx3RLMv"
   },
   "source": [
    "## **Sub-models**\n",
    "\n",
    "Next, we will define all sub-models in BERTopic and do some small tweaks to the number of clusters to be created, setting random states, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 463104,
     "status": "aborted",
     "timestamp": 1764126629931,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "2V1o0kcLRLMv"
   },
   "outputs": [],
   "source": [
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mb46htIKRLMv"
   },
   "source": [
    "As a small bonus, we are going to reduce the embeddings we created before to 2-dimensions so that we can use them for visualization purposes when we have created our topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdZCvn2LRLMv"
   },
   "source": [
    "### **Representation Models**\n",
    "\n",
    "One of the ways we are going to represent the topics is with Llama 2 which should give us a nice label. However, we might want to have additional representations to view a topic from multiple angles.\n",
    "\n",
    "Here, we will be using c-TF-IDF as our main representation and [KeyBERT](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#keybertinspired), [MMR](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#maximalmarginalrelevance), and [Llama 2](https://maartengr.github.io/BERTopic/getting_started/representation/llm.html) as our additional representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 463102,
     "status": "aborted",
     "timestamp": 1764126629931,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "4ccdqNWMRLMv"
   },
   "outputs": [],
   "source": [
    "# KeyBERT\n",
    "keybert = KeyBERTInspired()\n",
    "\n",
    "# MMR\n",
    "mmr = MaximalMarginalRelevance(diversity=0.3)\n",
    "\n",
    "# Text generation with Llama 2\n",
    "#llama2 = TextGeneration(generator, prompt=prompt)\n",
    "\n",
    "# All representation models\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert,\n",
    "    \"GPT-40\": ollama_representation_model, # Use the renamed object\n",
    "    \"MMR\": mmr,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GX2VIltRLMw"
   },
   "source": [
    "# ğŸ”¥ **Training**\n",
    "\n",
    "Now that we have our models prepared, we can start training our topic model! We supply BERTopic with the sub-models of interest, run `.fit_transform`, and see what kind of topics we get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Awk5apAPRLM3"
   },
   "source": [
    "## Multiple Representations\n",
    "During the development of BERTopic, many different types of representations can be created, from keywords and phrases to summaries and custom labels. There is a variety of techniques that one can choose from to represent a topic. As such, there are a number of interesting and creative ways one can summarize topics. A topic is more than just a single representation.\n",
    "\n",
    "Therefore, multi-aspect topic modeling is introduced! During the .fit or .fit_transform stages, you can now get multiple representations of a single topic. In practice, it works by generating and storing all kinds of different topic representations (see image below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 463102,
     "status": "aborted",
     "timestamp": 1764126629932,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "8oVGJcGZRLM3"
   },
   "outputs": [],
   "source": [
    "# To remove English stopwords\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "topic_model = BERTopic(\n",
    "\n",
    "  # Sub-models\n",
    "  embedding_model=embedding_model,\n",
    "  umap_model=umap_model,\n",
    "  hdbscan_model=hdbscan_model,\n",
    "  representation_model=representation_model,\n",
    "  vectorizer_model=vectorizer_model, # Add this line\n",
    "\n",
    "  # Hyperparameters\n",
    "  top_n_words=10,\n",
    "  verbose=True\n",
    ")\n",
    "\n",
    "# Train model\n",
    "topics, probs = topic_model.fit_transform(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3StbcscARLM3"
   },
   "source": [
    "Now that we are done training our model, let's see what topics were generated:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 463101,
     "status": "aborted",
     "timestamp": 1764126629933,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "gT-URHlIRLM3"
   },
   "outputs": [],
   "source": [
    "# Show topics\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 463101,
     "status": "aborted",
     "timestamp": 1764126629934,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "bTbTVp7jRLM3"
   },
   "outputs": [],
   "source": [
    "print(f\"Renderer set to '{pio.renderers.default}'\")\n",
    "fig = topic_model.visualize_topics()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 463101,
     "status": "aborted",
     "timestamp": 1764126629935,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "e28ZjGZhRLM3"
   },
   "outputs": [],
   "source": [
    "gpt4o_labels = [label[0][0].split(\"\\n\")[0] for label in topic_model.get_topics(full=True)[\"GPT-40\"].values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 463102,
     "status": "aborted",
     "timestamp": 1764126629937,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "91guyUt_6eQT"
   },
   "outputs": [],
   "source": [
    "def get_clean_label(raw_label_string):\n",
    "    \"\"\"Extracts clean label from list or string.\"\"\"\n",
    "    # If it's already a list, just take the first element\n",
    "    if isinstance(raw_label_string, list):\n",
    "        return raw_label_string[0] if raw_label_string else \"Unlabeled Topic\"\n",
    "\n",
    "    # If it's a string, clean it up\n",
    "    if isinstance(raw_label_string, str):\n",
    "        cleaned = raw_label_string.strip()\n",
    "        # Remove brackets and quotes if present\n",
    "        cleaned = cleaned.strip(\"[]\").strip().strip(\"'\\\"\").strip()\n",
    "        return cleaned if cleaned else \"Unlabeled Topic\"\n",
    "\n",
    "    # Fallback for other types\n",
    "    return str(raw_label_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 463101,
     "status": "aborted",
     "timestamp": 1764126629938,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "IKoxtPMURLM3"
   },
   "outputs": [],
   "source": [
    "# Get document info\n",
    "document_info = topic_model.get_document_info(docs)\n",
    "document_info[\"GPT-40\"] = document_info[\"GPT-40\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 463101,
     "status": "aborted",
     "timestamp": 1764126629939,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "vqRZ-Hh7RLM3"
   },
   "outputs": [],
   "source": [
    "# First, let's inspect the raw content of the 'GPT-40' column\n",
    "print(\"--- Raw GPT-40 labels (before cleaning) ---\")\n",
    "display(document_info[\"GPT-40\"].head())\n",
    "\n",
    "# Now, apply the cleaning function\n",
    "all_labels = document_info[\"GPT-40\"].apply(get_clean_label)\n",
    "\n",
    "print(\"\\n--- Cleaned Labels (after cleaning) ---\")\n",
    "display(all_labels.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 463101,
     "status": "aborted",
     "timestamp": 1764126629940,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "45SHaz-0RLM3"
   },
   "outputs": [],
   "source": [
    "fig = topic_model.visualize_barchart()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7npD2jORLM3"
   },
   "source": [
    "# Visualize Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 463100,
     "status": "aborted",
     "timestamp": 1764126629941,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "kGj-LxzvRLM3",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df_plot = pd.DataFrame({\n",
    "    \"x1\": [point[0] for point in reduced_embeddings],\n",
    "    \"x2\": [point[1] for point in reduced_embeddings],\n",
    "    \"docs\": docs,\n",
    "    \"label\": all_labels\n",
    "})\n",
    "df_plot[\"docs_short\"] = df_plot[\"docs\"].str[:100] + \"...\"\n",
    "df_plot.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 463099,
     "status": "aborted",
     "timestamp": 1764126629942,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "NB5WDjjHRLM3"
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(df_plot, x=\"x1\", y=\"x2\", color=\"label\", hover_data=[\"docs_short\"])\n",
    "\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    legend=dict(\n",
    "        orientation=\"h\",  # Change orientation to horizontal\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,           # Place the legend above the plot area\n",
    "        xanchor=\"right\",\n",
    "        x=1\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQllvmtfRLM3"
   },
   "source": [
    "Source: https://www.williampnicholson.com/2024-02-07-topic-modelling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 463097,
     "status": "aborted",
     "timestamp": 1764126629942,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "104j2SsMRLM3"
   },
   "outputs": [],
   "source": [
    "# Run the topic map visualization\n",
    "datamapplot.create_plot(\n",
    "    reduced_embeddings,\n",
    "    all_labels,\n",
    "\n",
    "    use_medoids=True,\n",
    "\n",
    "    # Follows matplotlibâ€™s 'figsize' convention.\n",
    "    # The actual size of the resulting plot (in pixels) will depend on the dots per inch (DPI)\n",
    "    # setting in matplotlib.\n",
    "    # By default that is set to 100 dots per inch for the standard backend, but it can vary.\n",
    "    figsize=(12, 12),\n",
    "    # If you really wish to have explicit control of the size of the resulting plot in pixels.\n",
    "    dpi=100,\n",
    "\n",
    "    title=\"arXiv cs.AI - Topic Analysis\",\n",
    "    sub_title=\"A Topic Map of arXiv's cs.AI sub-category based on abstracts from the arXiv api\",\n",
    "\n",
    "    # Takes a dictionary of keyword arguments that is passed through to\n",
    "    # matplotlibâ€™s 'suptitle' 'fontdict' arguments.\n",
    "    sub_title_keywords={\n",
    "        \"fontsize\":18,\n",
    "    },\n",
    "\n",
    "    # Takes a list of text labels to be highlighted.\n",
    "    # Note: these labels need to match the exact text from your labels array that you are passing in.\n",
    "    highlight_labels=[\n",
    "        \"Retinopathy Prematurity Screening\",\n",
    "    ],\n",
    "    # Takes a dictionary of keyword arguments to be applied when styling the labels.\n",
    "    highlight_label_keywords={\n",
    "        \"fontsize\": 12,\n",
    "        \"fontweight\": \"bold\",\n",
    "        \"bbox\": {\"boxstyle\":\"round\"}\n",
    "    },\n",
    "\n",
    "    # By default DataMapPlot tries to automatically choose a size for the text that will allow\n",
    "    # all the labels to be laid out well with no overlapping text. The layout algorithm will try\n",
    "    # to accommodate the size of the text you specify here.\n",
    "    label_font_size=8,\n",
    "    label_wrap_width=16,\n",
    "    label_linespacing=1.25,\n",
    "    # Default is 1.5. Generally, the values of 1.0 and 2.0 are the extremes.\n",
    "    # With 1.0 you will have more labels at the top and bottom.\n",
    "    # With 2.0 you will have more labels on the left and right.\n",
    "    label_direction_bias=1.3,\n",
    "    # Controls how large the margin is around the exact bounding box of a label, which is the\n",
    "    # bounding box used by the algorithm for collision/overlap detection.\n",
    "    # The default is 1.0, which means the margin is the same size as the label itself.\n",
    "    # Generally, the fewer labels you have the larger you can make the margin.\n",
    "    label_margin_factor=2.0,\n",
    "    # Labels are placed in rings around the core data map. This controls the starting radius for\n",
    "    # the first ring. Note: you need to provide a radius in data coordinates from the center of the\n",
    "    # data map.\n",
    "    # The defaul is selected from the data itself, based on the distance from the center of the\n",
    "    # most outlying points. Experiment and let the DataMapPlot algoritm try to clean it up.\n",
    "    label_base_radius=15.0,\n",
    "\n",
    "    # By default anything over 100,000 points uses datashader to create the scatterplot, while\n",
    "    # plots with fewer points use matplotlibâ€™s scatterplot.\n",
    "    # If DataMapPlot is using datashader then the point-size should be an integer,\n",
    "    # say 0, 1, 2, and possibly 3 at most. If however you are matplotlib scatterplot mode then you\n",
    "    # have a lot more flexibility in the point-size you can use - and in general larger values will\n",
    "    # be required. Experiment and see what works best.\n",
    "    point_size=4,\n",
    "\n",
    "    # Market type. There is only support if you are in matplotlib's scatterplot mode.\n",
    "    # https://matplotlib.org/stable/api/markers_api.html\n",
    "    marker_type=\"o\",\n",
    "\n",
    "    arrowprops={\n",
    "        \"arrowstyle\":\"wedge,tail_width=0.5\",\n",
    "        \"connectionstyle\":\"arc3,rad=0.05\",\n",
    "        \"linewidth\":0,\n",
    "        \"fc\":\"#33333377\"\n",
    "    },\n",
    "\n",
    "    add_glow=True,\n",
    "    # Takes a dictionary of keywords that are passed to the 'add_glow_to_scatterplot' function.\n",
    "    glow_keywords={\n",
    "        \"kernel_bandwidth\": 0.75,  # controls how wide the glow spreads.\n",
    "        \"kernel\": \"cosine\",        # controls the kernel type. Default is \"gaussian\". See https://scikit-learn.org/stable/modules/density.html#kernel-density.\n",
    "        \"n_levels\": 32,            # controls how many \"levels\" there are in the contour plot.\n",
    "        \"max_alpha\": 0.9,          # controls the translucency of the glow.\n",
    "    },\n",
    "\n",
    "    darkmode=False,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PDF, png, and svg file.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 463097,
     "status": "aborted",
     "timestamp": 1764126629943,
     "user": {
      "displayName": "Ryan Goshorn",
      "userId": "14943809824370629034"
     },
     "user_tz": 480
    },
    "id": "yEKN7HwyE13X"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
